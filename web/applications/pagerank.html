<html>
<head>
  <link rel="stylesheet" href="../plad.css">
  <script type="text/javascript" src="../plas.js"></script>
  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML' type='text/javascript'></script>
</head>

<body>
<h1>Pagerank</h1>

<p>
The pagerank of a webpage is defined to be the limit of the following process.
Suppose each webpage start with a unit of credit.
At each time unit, each webpage sends all its credit along its outgoing links, equally divided.
After many simulation steps, we will reach a fixed-point: a webpage gives all its credit, but receives back on its incoming links exactly how much it gave.

<p>
To be able to do this, we assume that each webpage has at least one outgoing link.
That's not true in practice.
The original Pagerank fixes this problem in a way that we leave as an exercise.
Instead, we use the following simple scheme:

<ul>
<li>add a dummy webpage
<li>add a link from each page (dummy or not) to the dummy
<li>add a link from the dummy to each non-dummy
</ul>

<h2>Input/Output</h2>

<p>
<b>Input:</b>
The first line contains an integer $n$ which gives the number of pages.
We will identify pages by numbers $1,\ldots,n$.
The second line contains a real number $\alpha$, which says what credit proportion is sent to the dummy page.
The next $n$ lines each contain a list of integers:
  on the $i$th line we have a list of pages to which page $i$ has links;
  this list is terminated by a dummy $0$.
It is possible to have multiple links to the same destination,
  and it is possible to have loops.

<p>
<b>Output:</b>
$n+1$ real numbers that add up to $n+1$.
The first is the pagerank of the dummy node; the following ones are the pageranks of pages $1,\ldots,n$.

<h2>Exercises</h2>

<ol>
<li>
  Our scheme is vulnerable to spammer spider traps.
  These are configurations of $1+n$ webpages $x,y_1,\ldots,y_n$
    with links $x \leftrightarrow y_k$ for all $k$.
  Because the dummy page has links to all of $y_1,\ldots,y_n$
    and they all link only to $x$, the latter ends up with a high pagerank.
  A solution is to add links from the dummy only to a subset of trusted nondummies.
  Implement this!
  And, by the way, the same idea is used to bias the search towards results from a certain topic.
<li>
  The original pagerank uses a different fix for the problem of draining credit.
  Let $t_{ij}$ be the fraction of $i$'s links that go to $j$,
    and write $T$ for the matrix with entries $t_{ij}$.
  Then, the simple version (with no fix), is asking for an $x$ such that $x=xT$.
  This can be solve by iterating, $x_{n+1}:=x_nT $.
  The original pagerank changes this to two steps:
    $y_n := x_n T$ and then $x_{n+1} := \alpha [1/n \ldots 1/n] + (1-\alpha) y_n$.
  Here, $\alpha\approx0.15$.
  So, the credit is first moved according to original links (with possible draininig),
    and then the credit of each page is shifted towards $1/n$.
  Try it!
</ol>

</body>
</html>
<!--
vim:spell:spelllang=en_us:wrap:linebreak:
-->
